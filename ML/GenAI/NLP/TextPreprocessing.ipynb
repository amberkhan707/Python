{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79778cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization krna h phle\n",
    "## Stop words htana h uske se\n",
    "## Root words nikalna h {saare method check krna h}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c94a5",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd363cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"My name is MD AMBER KHAN. I am 23 year old. I am a GenAI devloper.\n",
    "I am trying to do my best and i am the best.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86154243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is MD AMBER KHAN.',\n",
       " 'I am 23 year old.',\n",
       " 'I am a GenAI devloper.',\n",
       " 'I am trying to do my best and i am the best.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus -- > Documents tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e85d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'MD',\n",
       " 'AMBER',\n",
       " 'KHAN',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " '23',\n",
       " 'year',\n",
       " 'old',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'GenAI',\n",
       " 'devloper',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'do',\n",
       " 'my',\n",
       " 'best',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'the',\n",
       " 'best',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus -- > words tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159deab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'MD',\n",
       " 'AMBER',\n",
       " 'KHAN',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " '23',\n",
       " 'year',\n",
       " 'old',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'GenAI',\n",
       " 'devloper',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'do',\n",
       " 'my',\n",
       " 'best',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'the',\n",
       " 'best',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documents -- > words \n",
    "all_words = [word for sentence in documents for word in word_tokenize(sentence)]\n",
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d2e50",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2708e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_para = ['Going','History','Goes','Where','When','Ours','Hurriness','eaten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207b1cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'histori', 'goe', 'where', 'when', 'our', 'hurri', 'eaten']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#porterstemmer\n",
    "from nltk.stem import PorterStemmer \n",
    "porterstemmer = PorterStemmer()\n",
    "data1 = [porterstemmer.stem(word) for word in stem_para]\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dde03ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go', 'History', 'Go', 'Where', 'Wh', 'Our', 'Hurrines', 'eat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regexpstemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "regexpetemmer = RegexpStemmer('ing$|en$|es$|s$|')\n",
    "data2 = [regexpetemmer.stem(word) for word in stem_para]\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db35a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'histori', 'goe', 'where', 'when', 'our', 'hurri', 'eaten']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Snowballstemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer(\"english\")\n",
    "data3 = [snowballstemmer.stem(word) for word in stem_para]\n",
    "data3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d9edb",
   "metadata": {},
   "source": [
    "## Lemminization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc101c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mdamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fc0668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'history', 'go', 'where', 'when', 'ours', 'hurriness', 'eat']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "data4 = [wordnet.lemmatize(word.lower(),pos=\"v\") for word in stem_para]\n",
    "data4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03ac28",
   "metadata": {},
   "source": [
    "## Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bc86193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steve Jobs 2005 Stanford Commencement Speech Summary\n",
    "\n",
    "data = \"\"\"\n",
    "Steve Jobs, in his 2005 Stanford Commencement Address, shared three powerful life lessons through his personal stories.\n",
    "In the first story, 'Connecting the Dots', he explained how dropping out of college and following his curiosity for calligraphy\n",
    "later influenced the design of the Macintosh computer. His message was that we cannot connect the dots looking forward;\n",
    "we can only connect them looking backward, so we must trust in our intuition and path.\n",
    "\n",
    "In the second story, 'Love and Loss', Jobs talked about being fired from Apple, the company he founded.\n",
    "Though it was painful, it led him to create NeXT and Pixar, both of which became great successes.\n",
    "He learned that the only way to do great work is to love what you do.\n",
    "\n",
    "In the third story, 'Death', he reflected on his cancer diagnosis and how the awareness of death shaped his outlook on life.\n",
    "He emphasized that our time is limited, so we should not waste it living someone else’s life,\n",
    "nor let the noise of others’ opinions drown out our inner voice.\n",
    "\n",
    "He concluded his speech with the inspiring message:\n",
    "'Stay Hungry, Stay Foolish' — encouraging everyone to keep learning, exploring, and following their passion with courage and curiosity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a8f930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nSteve Jobs, in his 2005 Stanford Commencement Address, shared three powerful life lessons through his personal stories.',\n",
       " \"In the first story, 'Connecting the Dots', he explained how dropping out of college and following his curiosity for calligraphy\\nlater influenced the design of the Macintosh computer.\",\n",
       " 'His message was that we cannot connect the dots looking forward;\\nwe can only connect them looking backward, so we must trust in our intuition and path.',\n",
       " \"In the second story, 'Love and Loss', Jobs talked about being fired from Apple, the company he founded.\",\n",
       " 'Though it was painful, it led him to create NeXT and Pixar, both of which became great successes.',\n",
       " 'He learned that the only way to do great work is to love what you do.',\n",
       " \"In the third story, 'Death', he reflected on his cancer diagnosis and how the awareness of death shaped his outlook on life.\",\n",
       " 'He emphasized that our time is limited, so we should not waste it living someone else’s life,\\nnor let the noise of others’ opinions drown out our inner voice.',\n",
       " \"He concluded his speech with the inspiring message:\\n'Stay Hungry, Stay Foolish' — encouraging everyone to keep learning, exploring, and following their passion with courage and curiosity.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "documents = sent_tokenize(data)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a31c67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## count of word \n",
    "count = 0\n",
    "for i in range(len(documents)):\n",
    "    ls = [documents[i].split()]\n",
    "    x = len(ls[0])\n",
    "    count += x\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbe8c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mdamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ab0309d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steve Jobs , 2005 Stanford Commencement Address , shared three powerful life lesson personal story .',\n",
       " \"In first story , 'Connecting Dots ' , explained dropping college following curiosity calligraphy later influenced design Macintosh computer .\",\n",
       " 'His message connect dot looking forward ; connect looking backward , must trust intuition path .',\n",
       " \"In second story , 'Love Loss ' , Jobs talked fired Apple , company founded .\",\n",
       " 'Though painful , led create NeXT Pixar , became great success .',\n",
       " 'He learned way great work love .',\n",
       " \"In third story , 'Death ' , reflected cancer diagnosis awareness death shaped outlook life .\",\n",
       " 'He emphasized time limited , waste living someone else ’ life , let noise others ’ opinion drown inner voice .',\n",
       " \"He concluded speech inspiring message : 'Stay Hungry , Stay Foolish ' — encouraging everyone keep learning , exploring , following passion courage curiosity .\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    words = word_tokenize(documents[i])\n",
    "    words = [wordnet.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    documents[i] = ' '.join(words)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "133a88bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(documents)):\n",
    "    ls = [documents[i].split()]\n",
    "    x = len(ls[0])\n",
    "    count += x\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864a194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
